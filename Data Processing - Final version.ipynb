{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фрагментов тестовых данных: 29450\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import librosa \n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "from librosa.core import load\n",
    "\n",
    "\n",
    "data = [] # список для извлеченных тренировочных данных\n",
    "data_augmentation = [] # список для искуственно созданных данных\n",
    "data_test = []\n",
    "\n",
    "data_label = [] # список для меотк извлеченных тренировочных данных\n",
    "data_augmentation_label = [] # список для меток искуственно созданных данных\n",
    "data_test_label = []\n",
    "\n",
    "# переменные, нужные для аугментации данных\n",
    "#yTrain = 'beach bus cafe/restaurant car city_center forest_path grocery_store home library metro_station office park residential_area train tram'.split()\n",
    "yTrain = '1 2 3 4 5 6 7 8 9 10 11 12 13 14 15'.split()\n",
    "avallable_Spectrogram = []\n",
    "count = 0\n",
    "\n",
    "#data_train_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/mini_dataset/data_train/'\n",
    "#data_test_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/mini_dataset/data_for_test/'\n",
    "\n",
    "data_train_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for train/'\n",
    "data_test_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for test/'\n",
    "\n",
    "lambd = 0.5 \n",
    "idx = 0 # параметр для выбора рандомного значения индекса при смешивании\n",
    "n = 0\n",
    "score = 0 # параметр для вычисления количества элементов базе data \n",
    "add_line = [] # список для добавления искусственных данных\n",
    "new_data = []\n",
    "\n",
    "\n",
    "def transfrom_data(x, way): # функция извлечения данных \n",
    "    \n",
    "    eps = np.finfo(np.float32).eps # для логорифмирования\n",
    "    massive = [] # пустой массив для загрузкки каналов\n",
    "    massive_for_mfc = [] # пустой массив для загрузкки mfc\n",
    "    data10 = [] # пустой массив для загрузкки 10 секундный отрезков\n",
    "    n = 0 # локальная переменная для цикла\n",
    "    \n",
    "    way_2 = textik(x, way) # задаем путь через функцию textic\n",
    "    y, sr = librosa.load(way_2, sr=None, mono=False) \n",
    "    b=np.row_stack((np.sum(y,0), np.diff(y, axis=0))) # средняя кодировка\n",
    "    b = np.ascontiguousarray(b) \n",
    "    \n",
    "    # разделяем 1-й и 2-й каналы на секундные отрезки\n",
    "    b_patch1 = librosa.util.frame(b[0], frame_length=44100, hop_length=22050, axis = 0)\n",
    "    b_patch2 = librosa.util.frame(b[1], frame_length=44100, hop_length=22050, axis = 0)\n",
    "    \n",
    "    while n != 19:\n",
    "        # объеденяем два канала секундного отрезка n в один массив\n",
    "        massive.insert(0, b_patch1[n])\n",
    "        massive.insert(1, b_patch2[n])\n",
    "        massive = np.array(massive)\n",
    "        \n",
    "        # извлекаем mfc каждого из каналов\n",
    "        S1 = librosa.feature.melspectrogram(y=massive[0,:], sr=44100, n_fft = 2048, hop_length = 1024, win_length = 2048, n_mels=128, center = False)\n",
    "        S2 = librosa.feature.melspectrogram(y=massive[1,:], sr=44100, n_fft = 2048, hop_length = 1024, win_length = 2048, n_mels=128, center = False)\n",
    "        \n",
    "        # логорифмирование\n",
    "        S1 = np.log10(S1 + eps)\n",
    "        S2 = np.log10(S2 + eps)\n",
    "        \n",
    "        # загружаем извлеченные mfc в финальный массив\n",
    "        massive_for_mfc.insert(0, S1)\n",
    "        massive_for_mfc.insert(1, S2)\n",
    "        massive_for_mfc = np.array(massive_for_mfc)\n",
    "        massive_for_mfc = np.moveaxis(massive_for_mfc, [0,1,2], [2,0,1])\n",
    "        \n",
    "        data10.append(massive_for_mfc) # добавляем полученный 3-х мерный массив в возвращаемый массив\n",
    "        massive_for_mfc.tolist()\n",
    "        massive_for_mfc = []\n",
    "        massive = []\n",
    "        n +=1\n",
    "\n",
    "    return data10\n",
    "\n",
    "def textik(x, dir_way): # функция для определения директории \n",
    "    way = dir_way + str(x)\n",
    "    return way\n",
    "\n",
    "def augmentation(x_line,idx, mix): # функция-формула для расширения данных\n",
    "    mfc = mix[0]\n",
    "    #x = x_line[0] # функция работает для старого варианта - не использовать\n",
    "    x = x_line\n",
    "    new_data = lambd*x + (1-lambd)*mfc\n",
    "    return new_data\n",
    "\n",
    "# преобразование текстовой метки в численное значение для последущей векторизации\n",
    "def label_categorial(label):\n",
    "    if label == 'beach':\n",
    "        x = 0\n",
    "    elif label == 'bus':\n",
    "        x = 1\n",
    "    elif label == 'cafe/restaurant':\n",
    "        x = 2\n",
    "    elif label == 'car':\n",
    "        x = 3\n",
    "    elif label == 'city_center':\n",
    "        x = 4\n",
    "    elif label == 'forest_path':\n",
    "        x = 5\n",
    "    elif label == 'grocery_store':\n",
    "        x = 6\n",
    "    elif label == 'home':\n",
    "        x = 7\n",
    "    elif label == 'library':\n",
    "        x = 8\n",
    "    elif label == 'metro_station':\n",
    "        x = 9\n",
    "    elif label == 'office':\n",
    "        x = 10\n",
    "    elif label == 'park':\n",
    "        x = 11\n",
    "    elif label == 'residential_area':\n",
    "        x = 12\n",
    "    elif label == 'train':\n",
    "        x = 13\n",
    "    elif label == 'tram':\n",
    "        x = 14\n",
    "    else:\n",
    "        x = label\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "# обработка тестовых данных\n",
    "with open('/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for test/data_for_test.csv', 'r', encoding = 'utf-8-sig') as f:\n",
    "    csv_reader = csv.reader(f,delimiter=',')\n",
    "    for line in csv_reader:\n",
    "        n2 = 0\n",
    "        mark = label_categorial(line[1]) # метка \n",
    "        mark = str(mark)\n",
    "        data_with10 = transfrom_data(line[0], data_test_dir)\n",
    "        while n2 != 19:\n",
    "            data_test.append(data_with10[n2])\n",
    "            data_test_label.append(mark)\n",
    "            n2 +=1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "data_test = np.array(data_test)\n",
    "data_test_label = np.array(data_test_label)\n",
    "\n",
    "\n",
    "    \n",
    "with open('data_test.npy', 'wb') as f:\n",
    "    np.save(f, data_test, allow_pickle = False , fix_imports = False)\n",
    "    \n",
    "with open('data_test_label.npy', 'wb') as f:\n",
    "    np.save(f, data_test_label, allow_pickle = False , fix_imports = False)\n",
    "\n",
    "    \n",
    "print('Количество фрагментов тестовых данных:', len(data_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фрагментов тренировочных данных: 88920\n",
      "\n",
      "data.shape (88920, 128, 42, 2)\n",
      "data_label.shape (88920,)\n",
      "Количество фрагментов аугментироованных данных: 88920\n",
      "Итоговое количество фрагментов тренировочных данных: 177840\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import librosa \n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "from librosa.core import load\n",
    "\n",
    "\n",
    "data = [] # список для извлеченных тренировочных данных\n",
    "data_augmentation = [] # список для искуственно созданных данных\n",
    "data_test = []\n",
    "\n",
    "data_label = [] # список для меотк извлеченных тренировочных данных\n",
    "data_augmentation_label = [] # список для меток искуственно созданных данных\n",
    "data_test_label = []\n",
    "\n",
    "# переменные, нужные для аугментации данных\n",
    "#yTrain = 'beach bus cafe/restaurant car city_center forest_path grocery_store home library metro_station office park residential_area train tram'.split()\n",
    "yTrain = '1 2 3 4 5 6 7 8 9 10 11 12 13 14 15'.split()\n",
    "avallable_Spectrogram = []\n",
    "count = 0\n",
    "\n",
    "#data_train_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/mini_dataset/data_train/'\n",
    "#data_test_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/mini_dataset/data_for_test/'\n",
    "\n",
    "data_train_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for train/'\n",
    "data_test_dir = '/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for test/'\n",
    "\n",
    "lambd = 0.5 \n",
    "idx = 0 # параметр для выбора рандомного значения индекса при смешивании\n",
    "n = 0\n",
    "score = 0 # параметр для вычисления количества элементов базе data \n",
    "add_line = [] # список для добавления искусственных данных\n",
    "new_data = []\n",
    "\n",
    "\n",
    "def transfrom_data(x, way): # функция извлечения данных \n",
    "    \n",
    "    eps = np.finfo(np.float32).eps # для логорифмирования\n",
    "    massive = [] # пустой массив для загрузкки каналов\n",
    "    massive_for_mfc = [] # пустой массив для загрузкки mfc\n",
    "    data10 = [] # пустой массив для загрузкки 10 секундный отрезков\n",
    "    n = 0 # локальная переменная для цикла\n",
    "    \n",
    "    way_2 = textik(x, way) # задаем путь через функцию textic\n",
    "    y, sr = librosa.load(way_2, sr=None, mono=False) \n",
    "    b=np.row_stack((np.sum(y,0), np.diff(y, axis=0))) # средняя кодировка\n",
    "    b = np.ascontiguousarray(b) \n",
    "    \n",
    "    # разделяем 1-й и 2-й каналы на секундные отрезки\n",
    "    b_patch1 = librosa.util.frame(b[0], frame_length=44100, hop_length=22050, axis = 0)\n",
    "    b_patch2 = librosa.util.frame(b[1], frame_length=44100, hop_length=22050, axis = 0)\n",
    "    \n",
    "    while n != 19:\n",
    "        # объеденяем два канала секундного отрезка n в один массив\n",
    "        massive.insert(0, b_patch1[n])\n",
    "        massive.insert(1, b_patch2[n])\n",
    "        massive = np.array(massive)\n",
    "        \n",
    "        # извлекаем mfc каждого из каналов\n",
    "        S1 = librosa.feature.melspectrogram(y=massive[0,:], sr=44100, n_fft = 2048, hop_length = 1024, win_length = 2048, n_mels=128, center = False)\n",
    "        S2 = librosa.feature.melspectrogram(y=massive[1,:], sr=44100, n_fft = 2048, hop_length = 1024, win_length = 2048, n_mels=128, center = False)\n",
    "        \n",
    "        # логорифмирование\n",
    "        S1 = np.log10(S1 + eps)\n",
    "        S2 = np.log10(S2 + eps)\n",
    "        \n",
    "        # загружаем извлеченные mfc в финальный массив\n",
    "        massive_for_mfc.insert(0, S1)\n",
    "        massive_for_mfc.insert(1, S2)\n",
    "        massive_for_mfc = np.array(massive_for_mfc)\n",
    "        massive_for_mfc = np.moveaxis(massive_for_mfc, [0,1,2], [2,0,1])\n",
    "        \n",
    "        data10.append(massive_for_mfc) # добавляем полученный 3-х мерный массив в возвращаемый массив\n",
    "        massive_for_mfc.tolist()\n",
    "        massive_for_mfc = []\n",
    "        massive = []\n",
    "        n +=1\n",
    "\n",
    "    return data10\n",
    "\n",
    "def textik(x, dir_way): # функция для определения директории \n",
    "    way = dir_way + str(x)\n",
    "    return way\n",
    "\n",
    "def augmentation(x_line,idx, mix): # функция-формула для расширения данных\n",
    "    mfc = mix\n",
    "    mfc = mix[0] # функция работает для старого варианта - не использовать\n",
    "    #x = x_line[0] # функция работает для старого варианта - не использовать\n",
    "    x = x_line\n",
    "    new_data = lambd*x + (1-lambd)*mfc\n",
    "    return new_data\n",
    "\n",
    "# преобразование текстовой метки в численное значение для последущей векторизации\n",
    "def label_categorial(label):\n",
    "    if label == 'beach':\n",
    "        x = 0\n",
    "    elif label == 'bus':\n",
    "        x = 1\n",
    "    elif label == 'cafe/restaurant':\n",
    "        x = 2\n",
    "    elif label == 'car':\n",
    "        x = 3\n",
    "    elif label == 'city_center':\n",
    "        x = 4\n",
    "    elif label == 'forest_path':\n",
    "        x = 5\n",
    "    elif label == 'grocery_store':\n",
    "        x = 6\n",
    "    elif label == 'home':\n",
    "        x = 7\n",
    "    elif label == 'library':\n",
    "        x = 8\n",
    "    elif label == 'metro_station':\n",
    "        x = 9\n",
    "    elif label == 'office':\n",
    "        x = 10\n",
    "    elif label == 'park':\n",
    "        x = 11\n",
    "    elif label == 'residential_area':\n",
    "        x = 12\n",
    "    elif label == 'train':\n",
    "        x = 13\n",
    "    elif label == 'tram':\n",
    "        x = 14\n",
    "    else:\n",
    "        x = label\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('/Users/sedoy/Desktop/Work Files/Программирование/Нейросеть/data for train/data_train_way .csv', 'r', encoding = 'utf-8-sig') as f:\n",
    "    csv_reader = csv.reader(f,delimiter=',')\n",
    "    for line in csv_reader:\n",
    "        n2 = 0\n",
    "        mark = label_categorial(line[1]) # метка \n",
    "        mark = str(mark)        \n",
    "        data_with10 = transfrom_data(line[0], data_train_dir)\n",
    "        while n2 != 19:\n",
    "            data.append(data_with10[n2])\n",
    "            data_label.append(mark)\n",
    "            n2 +=1\n",
    "            score +=1\n",
    "    \n",
    "    \n",
    "print('Количество фрагментов тренировочных данных:', len(data))\n",
    "print()\n",
    "print('data.shape', np.array(data).shape)\n",
    "print('data_label.shape', np.array(data_label).shape)\n",
    "\n",
    "\n",
    "spec = np.array(data)\n",
    "xTrainExtra=np.array(data)\n",
    "yTrainExtra=np.array(data_label)\n",
    "lamda=.5\n",
    "yT=np.array(data_label)\n",
    "\n",
    "\n",
    "for q in range (0,spec.shape[0]):\n",
    "    a=yT[q]\n",
    "    availableSpectrograms=np.argwhere(yT != a)\n",
    "    numAvailableSpectrograms= len(availableSpectrograms)\n",
    "    idx=random.randint(0,numAvailableSpectrograms-1)\n",
    "    xTrainExtra[q,:,:,:]=lamda*spec[q,:,:,:]+(1-lamda)*spec[availableSpectrograms[idx,0],:,:,:]\n",
    "    \n",
    "    if random.random()>lamda:\n",
    "        yTrainExtra[q]=yT[availableSpectrograms[idx,0]]\n",
    "        \n",
    "xTrainFull=np.concatenate((spec, xTrainExtra)) \n",
    "yTrainFull=np.concatenate((yT, yTrainExtra))\n",
    "\n",
    "\n",
    "with open('data_aug.npy', 'wb') as f:\n",
    "    np.save(f, xTrainFull, allow_pickle = False , fix_imports = False)\n",
    "    \n",
    "with open('data_label_aug.npy', 'wb') as f:\n",
    "    np.save(f, yTrainFull, allow_pickle = False , fix_imports = False)\n",
    "    \n",
    "\n",
    "    \n",
    "print('Количество фрагментов аугментироованных данных:', len(xTrainExtra))\n",
    "print('Итоговое количество фрагментов тренировочных данных:', len(xTrainFull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
